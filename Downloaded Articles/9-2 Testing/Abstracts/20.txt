Abstract
How do large language models (LLMs) develop
and evolve over the course of training? How do
these patterns change as models scale? To an-
swer these questions, we introduce Pythia, a suite
of 16 LLMs all trained on public data seen in
the exact same order and ranging in size from
70M to 12B parameters. We provide public ac-
cess to 154 checkpoints for each one of the 16
models, alongside tools to download and recon-
struct their exact training dataloaders for further
study. We intend Pythia to facilitate research in
many areas, and we present several case stud-
ies including novel results in memorization, term
frequency effects on few-shot performance, and
reducing gender bias. We demonstrate that this
highly controlled setup can be used to yield novel
insights toward LLMs and their training dynam-
ics.
Trained models, analysis code, training
code, and training data can be found at https:
//github.com/EleutherAI/pythia.
1.