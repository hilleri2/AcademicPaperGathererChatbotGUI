[{"title": "ChatGPT for good? On opportunities and challenges of large language models for education", "link": "https://www.sciencedirect.com/science/article/pii/S1041608023000195", "file_link": "https://epub.ub.uni-muenchen.de/125071/1/ChatGPT_for_Good_v3.pdf", "authors": ["E Kasneci, K Se\u00dfler, S K\u00fcchemann, M Bannert\u2026", "- Learning and individual", "\u2026, 2023 - Elsevier"], "scholar_id": "uthwmf2nU3EJ", "snippet": "\u2026 state of large language models and their applications. We then highlight how these models can be \u2026 With regard to challenges, we argue that large language models in education require \u2026"}, {"title": "A survey on evaluation of large language models", "link": "https://dl.acm.org/doi/abs/10.1145/3641289", "file_link": "https://dl.acm.org/doi/pdf/10.1145/3641289", "authors": ["Y Chang, X Wang, J Wang, Y Wu, L Yang\u2026", "- ACM transactions on", "\u2026, 2024 - dl.acm.org"], "scholar_id": "o93zfHYlUTIJ", "snippet": "\u2026 3.1 Natural Language Processing Tasks \u2026 the development of language models, particularly large language models, was to enhance performance on natural language processing tasks, \u2026"}, {"title": "A comprehensive overview of large language models", "link": "https://dl.acm.org/doi/abs/10.1145/3744746", "file_link": "https://arxiv.org/pdf/2307.06435", "authors": ["H Naveed, AU Khan, S Qiu, M Saqib, S Anwar\u2026", "- ACM Transactions on", "\u2026, 2025 - dl.acm.org"], "scholar_id": "UDLkJGuOVl4J", "snippet": "\u2026 Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of \u2026"}, {"title": "[PDF][PDF] A survey of large language models", "link": "https://www.researchgate.net/profile/Tang-Tianyi-3/publication/369740832_A_Survey_of_Large_Language_Models/links/665fd2e3637e4448a37dd281/A-Survey-of-Large-Language-Models.pdf", "file_link": "https://www.researchgate.net/profile/Tang-Tianyi-3/publication/369740832_A_Survey_of_Large_Language_Models/links/665fd2e3637e4448a37dd281/A-Survey-of-Large-Language-Models.pdf", "authors": ["WX Zhao, K Zhou, J Li, T Tang, X Wang\u2026", "- arXiv preprint arXiv", "\u2026, 2023 - researchgate.net"], "scholar_id": "Laj3JsRSUToJ", "snippet": "\u2026 small-scale language models (eg, BERT). To discriminate the language models in different parameter scales, the research community has coined the term large language models (LLM) \u2026"}, {"title": "No title", "link": null, "file_link": null, "authors": "No authors", "scholar_id": "No ID", "snippet": "No snippet"}, {"title": "Large language models in medicine", "link": "https://www.nature.com/articles/s41591-023-02448-8", "file_link": "https://drive.google.com/file/d/1FKEGsSZ9GYOeToeKpxB4m3atGRbC-TSm/view", "authors": ["AJ Thirunavukarasu, DSJ Ting, K Elangovan\u2026", "- Nature medicine, 2023 - nature.com"], "scholar_id": "Ph9AwHTmhzAJ", "snippet": "\u2026 HuggingChat offers a free-to-access chatbot with a similar interface to ChatGPT but uses Large Language Model Meta AI (LLaMA) as its backend model 30 . Finally, cheap imitations of \u2026"}, {"title": "A watermark for large language models", "link": "https://proceedings.mlr.press/v202/kirchenbauer23a.html", "file_link": "https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf", "authors": ["J Kirchenbauer, J Geiping, Y Wen\u2026", "- International", "\u2026, 2023 - proceedings.mlr.press"], "scholar_id": "BlSyLHT4iiEJ", "snippet": "\u2026 We propose a watermarking framework for proprietary language models. The \u2026 in the language model just before it produces a probability vector. The last layer of the language model \u2026"}, {"title": "Talking about large language models", "link": "https://dl.acm.org/doi/abs/10.1145/3624724", "file_link": "https://dl.acm.org/doi/pdf/10.1145/3624724", "authors": ["M Shanahan", "- Communications of the ACM, 2024 - dl.acm.org"], "scholar_id": "3eYYI745r_0J", "snippet": "\u2026 Recently, it has become commonplace to use the term \u201clarge language model\u201d both for the generative models themselves and for the systems in which they are embedded, especially in\u00a0\u2026"}, {"title": "Welcome to the era of chatgpt et al. the prospects of large language models", "link": "https://link.springer.com/article/10.1007/s12599-023-00795-x", "file_link": "https://link.springer.com/content/pdf/10.1007/s12599-023-00795-x.pdf", "authors": ["T Teubner, CM Flath, C Weinhardt\u2026", "- Business & Information", "\u2026, 2023 - Springer"], "scholar_id": "3UrgC1BmpV8J", "snippet": "The emergence of Large Language Models (LLMs) in combination with easy-to-use interfaces such as ChatGPT, Bing Chat, and Google\u2019s Bard represent both a Herculean task and a \u2026"}, {"title": "Large language models: A survey", "link": "https://arxiv.org/abs/2402.06196", "file_link": "https://arxiv.org/pdf/2402.06196", "authors": ["S Minaee, T Mikolov, N Nikzad, M Chenaghlu\u2026", "- arXiv preprint arXiv", "\u2026, 2024 - arxiv.org"], "scholar_id": "msVvatpT9iUJ", "snippet": "\u2026 large language models with our #parameters threshold, we included a few representative works, which pushed the limits of language models\u2026 as some small language models. \u2663 shows \u2026"}, {"title": "A systematic evaluation of large language models of code", "link": "https://dl.acm.org/doi/abs/10.1145/3520312.3534862", "file_link": "https://dl.acm.org/doi/pdf/10.1145/3520312.3534862", "authors": ["FF Xu, U Alon, G Neubig, VJ Hellendoorn", "- Proceedings of the 6th ACM", "\u2026, 2022 - dl.acm.org"], "scholar_id": "-iQSW0h72hYJ", "snippet": "\u2026 largest language models for code. We also release PolyCoder, a large open-source language model for code, trained exclusively on code in 12 different programming languages. In the \u2026"}, {"title": "Challenges and applications of large language models", "link": "https://arxiv.org/abs/2307.10169", "file_link": "https://arxiv.org/pdf/2307.10169", "authors": ["J Kaddour, J Harris, M Mozes, H Bradley\u2026", "- arXiv preprint arXiv", "\u2026, 2023 - arxiv.org"], "scholar_id": "YCygM0k2CoYJ", "snippet": "\u2026 We start with the most basic and still widelyused Language Modeling [59] (or next token prediction) objective. Here, we learn parameters \u03b8 by maximizing the likelihood of the next token \u2026"}, {"title": "Emergent abilities of large language models", "link": "https://arxiv.org/abs/2206.07682", "file_link": "https://arxiv.org/pdf/2206.07682", "authors": ["J Wei, Y Tay, R Bommasani, C Raffel, B Zoph\u2026", "- arXiv preprint arXiv", "\u2026, 2022 - arxiv.org"], "scholar_id": "hG0iVOrOguoJ", "snippet": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an \u2026"}, {"title": "Large language models can self-improve", "link": "https://arxiv.org/abs/2210.11610", "file_link": "https://arxiv.org/pdf/2210.11610", "authors": ["J Huang, SS Gu, L Hou, Y Wu, X Wang, H Yu\u2026", "- arXiv preprint arXiv", "\u2026, 2022 - arxiv.org"], "scholar_id": "Pmy39Yp6r5gJ", "snippet": "\u2026 \u2022 We demonstrate that a large language model can self-improve by taking datasets without ground truth outputs, by leveraging CoT reasoning (Wei et al., 2022b) and selfconsistency (\u2026"}, {"title": "Eight things to know about large language models", "link": "https://read.dukeupress.edu/critical-ai/article-abstract/doi/10.1215/2834703X-11556011/400182", "file_link": "https://read.dukeupress.edu/critical-ai/article/doi/10.1215/2834703X-11556011/400182", "authors": ["SR Bowman", "- Critical AI, 2024 - read.dukeupress.edu"], "scholar_id": "qYli5Mz6xAUJ", "snippet": "\u2026 and analysis tools for large language models. His research focuses primarily on developing techniques and datasets for use in controlling and evaluating large language models, and \u2026"}, {"title": "Autoformalization with large language models", "link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/d0c6bc641a56bebee9d985b937307367-Abstract-Conference.html", "file_link": "https://proceedings.neurips.cc/paper_files/paper/2022/file/d0c6bc641a56bebee9d985b937307367-Paper-Conference.pdf", "authors": ["Y Wu, AQ Jiang, W Li, M Rabe\u2026", "- Advances in neural", "\u2026, 2022 - proceedings.neurips.cc"], "scholar_id": "J6e5D10SvdUJ", "snippet": "\u2026 While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising \u2026"}, {"title": "[HTML][HTML] Summary of chatgpt-related research and perspective towards the future of large language models", "link": "https://www.sciencedirect.com/science/article/pii/S2950162823000176", "file_link": "https://www.sciencedirect.com/science/article/pii/S2950162823000176", "authors": ["Y Liu, T Han, S Ma, J Zhang, Y Yang, J Tian, H He, A Li\u2026", "- Meta-radiology, 2023 - Elsevier"], "scholar_id": "9b297h0ci9oJ", "snippet": "This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and GPT-4) research, state-of-the-art large language models (LLM) from the GPT series, and their \u2026"}, {"title": "Evaluating large language models trained on code", "link": "https://arxiv.org/abs/2107.03374", "file_link": "https://arxiv.org/pdf/2107.03374", "authors": ["M Chen, J Tworek, H Jun, Q Yuan, HPDO Pinto\u2026", "- arXiv preprint arXiv", "\u2026, 2021 - arxiv.org"], "scholar_id": "3tNvW3l5nU4J", "snippet": "\u2026 We introduce Codex, a GPT language model finetuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex \u2026"}, {"title": "Aligning large language models with human: A survey", "link": "https://arxiv.org/abs/2307.12966", "file_link": "https://arxiv.org/pdf/2307.12966", "authors": ["Y Wang, W Zhong, L Li, F Mi, X Zeng, W Huang\u2026", "- arXiv preprint arXiv", "\u2026, 2023 - arxiv.org"], "scholar_id": "5_egaBXZVSYJ", "snippet": "\u2026 In this survey, we aim to provide a comprehensive overview of alignment technologies for large language models. In Section 2, we summarize various methods in effective high-quality \u2026"}, {"title": "Do large language models understand us?", "link": "https://direct.mit.edu/daed/article-abstract/151/2/183/110604", "file_link": "https://direct.mit.edu/daed/article-pdf/151/2/183/2060575/daed_a_01909.pdf", "authors": ["BA y Arcas", "- Daedalus, 2022 - direct.mit.edu"], "scholar_id": "PKqFujsrsK0J", "snippet": "\u2026 of large language models. Out of necessity, though, LaMDA's creators have taken a small step in this direction by having the model \u2026 stage uses the same large language model; so the \u2026"}, {"title": "[HTML][HTML] Large language models encode clinical knowledge", "link": "https://www.nature.com/articles/s41586-023-06291-2).", "file_link": "https://www.nature.com/articles/s41586-023-06291-2).", "authors": ["K Singhal, S Azizi, T Tu, SS Mahdavi, J Wei, HW Chung\u2026", "- Nature, 2023 - nature.com"], "scholar_id": "kJdgqZdRQnwJ", "snippet": "Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely \u2026"}, {"title": "Large language models meet NL2Code: A survey", "link": "https://arxiv.org/abs/2212.09420", "file_link": "https://arxiv.org/pdf/2212.09420", "authors": ["D Zan, B Chen, F Zhang, D Lu, B Wu, B Guan\u2026", "- arXiv preprint arXiv", "\u2026, 2022 - arxiv.org"], "scholar_id": "yHjCjKTRC20J", "snippet": "\u2026 large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models \u2026 survey of large language models for NL2Code\u2026"}, {"title": "Using large language models in psychology", "link": "https://www.nature.com/articles/s44159-023-00241-5", "file_link": "https://par.nsf.gov/servlets/purl/10575508", "authors": ["D Demszky, D Yang, DS Yeager, CJ Bryan\u2026", "- Nature Reviews", "\u2026, 2023 - nature.com"], "scholar_id": "IOa0oC5AOIkJ", "snippet": "Large language models (LLMs), such as OpenAI\u2019s GPT-4, Google\u2019s Bard or Meta\u2019s LLaMa, have created unprecedented opportunities for analysing and generating language data on a \u2026"}, {"title": "A survey on multimodal large language models", "link": "https://academic.oup.com/nsr/advance-article/doi/10.1093/nsr/nwae403/7896414", "file_link": "https://academic.oup.com/nsr/advance-article-pdf/doi/10.1093/nsr/nwae403/61201557/nwae403.pdf", "authors": ["S Yin, C Fu, S Zhao, K Li, X Sun, T Xu\u2026", "- National Science", "\u2026, 2024 - academic.oup.com"], "scholar_id": "u0n2Cx-TnhwJ", "snippet": "\u2026 Recently, the multimodal large language model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful large language models (LLMs) as a brain \u2026"}, {"title": "Role play with large language models", "link": "https://www.nature.com/articles/s41586-023-06647-8", "file_link": "https://arxiv.org/pdf/2305.16367", "authors": ["M Shanahan, K McDonell, L Reynolds", "- Nature, 2023 - nature.com"], "scholar_id": "tMltOHQr9KAJ", "snippet": "\u2026 term \u2018large language model\u2019 tends to be reserved for transformer-based models that have \u2026 This blog post introduced the idea that a large language model maintains a set of simulated \u2026"}, {"title": "Can large language models reason about medical questions?", "link": "https://www.cell.com/patterns/fulltext/S2666-3899(24)00042-4", "file_link": "https://www.cell.com/patterns/pdfExtended/S2666-3899(24)00042-4", "authors": ["V Li\u00e9vin, CE Hother, AG Motzfeldt, O Winther", "- Patterns, 2024 - cell.com"], "scholar_id": "0pxn5pYal2gJ", "snippet": "\u2026 Foundation models have changed the way machine learning is practiced. Foundation models applied to text, so-called large language models (LLMs), have proven to be a disruptive \u2026"}, {"title": "ChatGPT and large language models in academia: opportunities and challenges", "link": "https://link.springer.com/article/10.1186/s13040-023-00339-9", "file_link": "https://link.springer.com/content/pdf/10.1186/s13040-023-00339-9.pdf", "authors": ["JG Meyer, RJ Urbanowicz, PCN Martin, K O'Connor\u2026", "- BioData mining, 2023 - Springer"], "scholar_id": "Gn2oKgMDg48J", "snippet": "\u2026 Thus, an answer generated by a large language model can be formatted correctly but not necessarily be factual. One needs to remember that it is not a human: it's not trained to respond \u2026"}, {"title": "Large language models as general pattern machines", "link": "https://arxiv.org/abs/2307.04721", "file_link": "https://arxiv.org/pdf/2307.04721", "authors": ["S Mirchandani, F Xia, P Florence, B Ichter\u2026", "- arXiv preprint arXiv", "\u2026, 2023 - arxiv.org"], "scholar_id": "ZMDsEMrlrYcJ", "snippet": "\u2026 Abstract: We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences\u2014from arbitrary ones procedurally generated \u2026"}, {"title": "When large language models meet personalization: Perspectives of challenges and opportunities", "link": "https://link.springer.com/article/10.1007/s11280-024-01276-1", "file_link": "https://link.springer.com/content/pdf/10.1007/s11280-024-01276-1.pdf", "authors": ["J Chen, Z Liu, X Huang, C Wu, Q Liu, G Jiang, Y Pu\u2026", "- World Wide Web, 2024 - Springer"], "scholar_id": "-_Vb7ILKzFMJ", "snippet": "\u2026 of large language models in \u2026 large language models in Section 2 to overview the development and challenges. Then we carefully discuss the potential actors of large language models \u2026"}, {"title": "A survey on large language models for recommendation", "link": "https://link.springer.com/article/10.1007/s11280-024-01291-2", "file_link": "https://arxiv.org/pdf/2305.19860", "authors": ["L Wu, Z Zheng, Z Qiu, H Wang, H Gu, T Shen, C Qin\u2026", "- World Wide Web, 2024 - Springer"], "scholar_id": "zZAMcbt-n6AJ", "snippet": "\u2026 paradigms, we categorize the current studies of large language model recommendations into three distinct schools of thought. Any existing method can be fittingly placed within these \u2026"}, {"title": "Risks and benefits of large language models for the environment", "link": "https://pubs.acs.org/doi/full/10.1021/acs.est.3c01106", "file_link": "https://pubs.acs.org/doi/pdf/10.1021/acs.est.3c01106", "authors": ["MC Rillig, M \u00c5gerstrand, M Bi, KA Gould\u2026", "- \u2026", "science & technology, 2023 - ACS Publications"], "scholar_id": "6XseJssriE8J", "snippet": "\u2026 Large language models come with risks and opportunities for the environment. Increased use of large language models could affect the environment positively or negatively, with \u2026"}, {"title": "[HTML][HTML] A review of current trends, techniques, and challenges in large language models (llms)", "link": "https://www.mdpi.com/2076-3417/14/5/2074", "file_link": "https://www.mdpi.com/2076-3417/14/5/2074", "authors": ["R Patil, V Gudivada", "- Applied Sciences, 2024 - mdpi.com"], "scholar_id": "nCXIXJ7Lx88J", "snippet": "\u2026 In Section 2, we look at the language model definition and the attention layer mechanism in detail. In Section 3, we describe the types of architectures and attention masks used in \u2026"}, {"title": "A survey on model compression for large language models", "link": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00704/125482", "file_link": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00704/2482209/tacl_a_00704.pdf", "authors": ["X Zhu, J Li, Y Liu, C Ma, W Wang", "- Transactions of the Association for", "\u2026, 2024 - direct.mit.edu"], "scholar_id": "hy-dUbcYbhoJ", "snippet": "Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, \u2026"}, {"title": "Training compute-optimal large language models", "link": "https://arxiv.org/abs/2203.15556", "file_link": "https://arxiv.org/pdf/2203.15556", "authors": ["J Hoffmann, S Borgeaud, A Mensch\u2026", "- arXiv preprint arXiv", "\u2026, 2022 - arxiv.org"], "scholar_id": "wEmD6BMp1T4J", "snippet": "\u2026 We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of \u2026"}, {"title": "Science in the age of large language models", "link": "https://www.nature.com/articles/s42254-023-00581-4", "file_link": null, "authors": ["A Birhane, A Kasirzadeh, D Leslie\u2026", "- Nature Reviews Physics, 2023 - nature.com"], "scholar_id": "pdjksBsNwjEJ", "snippet": "\u2026 the capabilities of large language models and the \u2026 Large language models (LLMs) are deep learning models with a huge number of parameters trained in an unsupervised way on large \u2026"}, {"title": "[HTML][HTML] Opportunities and challenges for ChatGPT and large language models in biomedicine and health", "link": "https://journals.aai.org/bib/article/25/1/bbad493/7505071", "file_link": "https://journals.aai.org/bib/article/25/1/bbad493/7505071", "authors": ["S Tian, Q Jin, L Yeganova, PT Lai, Q Zhu\u2026", "- Briefings in", "\u2026, 2023 - journals.aai.org"], "scholar_id": "AY_DHT1rZD4J", "snippet": "\u2026 In this work, we examine the diverse applications of large language models (LLMs), such as ChatGPT, in biomedicine and health. Specifically, we explore the areas of biomedical \u2026"}, {"title": "The future landscape of large language models in medicine", "link": "https://www.nature.com/articles/s43856-023-00370-1", "file_link": "https://www.nature.com/articles/s43856-023-00370-1.pdf", "authors": ["J Clusmann, FR Kolbinger, HS Muti, ZI Carrero\u2026", "- Communications", "\u2026, 2023 - nature.com"], "scholar_id": "rFLBW9M5kd0J", "snippet": "\u2026 The implications of large language models for medical education and knowledge \u2026 Prompt programming for large language models: beyond the few-shot paradigm. In Extended Abstracts \u2026"}, {"title": "Towards reasoning in large language models: A survey", "link": "https://arxiv.org/abs/2212.10403", "file_link": "https://arxiv.org/pdf/2212.10403", "authors": ["J Huang, KCC Chang", "- arXiv preprint arXiv:2212.10403, 2022 - arxiv.org"], "scholar_id": "8PE5MQUdMLUJ", "snippet": "\u2026 years, large language models (LLMs) have made significant progress in natural language \u2026 state of knowledge on reasoning in large language models. Reasoning is a broad concept \u2026"}, {"title": "Pythia: A suite for analyzing large language models across training and scaling", "link": "https://proceedings.mlr.press/v202/biderman23a.html", "file_link": "https://proceedings.mlr.press/v202/biderman23a/biderman23a.pdf", "authors": ["S Biderman, H Schoelkopf\u2026", "- International", "\u2026, 2023 - proceedings.mlr.press"], "scholar_id": "aaIDvsMAD8QJ", "snippet": "\u2026 large language models, we prioritize consistency in model \u2026 out the most performance from each model. For example, we \u2026 models, as it is becoming widely used for the largest models, \u2026"}, {"title": "Wordcraft: story writing with large language models", "link": "https://dl.acm.org/doi/abs/10.1145/3490099.3511105", "file_link": "https://dl.acm.org/doi/pdf/10.1145/3490099.3511105", "authors": ["A Yuan, A Coenen, E Reif, D Ippolito", "- Proceedings of the 27th", "\u2026, 2022 - dl.acm.org"], "scholar_id": "XjSGH5_5fQ4J", "snippet": "\u2026 large language models enable novel co-writing experiences. For example, the language model \u2026 to writers\u2019 custom requests expressed in natural language (such as \"rewrite this text to \u2026"}, {"title": "Large language models in finance: A survey", "link": "https://dl.acm.org/doi/abs/10.1145/3604237.3626869", "file_link": "https://dl.acm.org/doi/pdf/10.1145/3604237.3626869", "authors": ["Y Li, S Wang, H Ding, H Chen", "- \u2026", "ACM international conference on AI in", "\u2026, 2023 - dl.acm.org"], "scholar_id": "U0-FE9kRKxgJ", "snippet": "Recent advances in large language models (LLMs) have opened new possibilities for artificial intelligence applications in finance. In this paper, we provide a practical survey focused \u2026"}, {"title": "Detecting pretraining data from large language models", "link": "https://arxiv.org/abs/2310.16789", "file_link": "https://arxiv.org/pdf/2310.16789", "authors": ["W Shi, A Ajith, M Xia, Y Huang, D Liu, T Blevins\u2026", "- arXiv preprint arXiv", "\u2026, 2023 - arxiv.org"], "scholar_id": "2yufYZAea2oJ", "snippet": "\u2026 This is not possible for large language models, as the training distribution is usually not available \u2026 Pythia: A suite for analyzing large language models across training and scaling, 2023. \u2026"}, {"title": "Wizardcoder: Empowering code large language models with evol-instruct", "link": "https://arxiv.org/abs/2306.08568", "file_link": "https://arxiv.org/pdf/2306.08568", "authors": ["Z Luo, C Xu, P Zhao, Q Sun, X Geng, W Hu\u2026", "- arXiv preprint arXiv", "\u2026, 2023 - arxiv.org"], "scholar_id": "KDFQqLvnZwYJ", "snippet": "\u2026 model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest \u2026 Our code, model weights, and data are public at https://\u2026"}, {"title": "Large language models for information retrieval: A survey", "link": "https://arxiv.org/abs/2308.07107", "file_link": "https://arxiv.org/pdf/2308.07107", "authors": ["Y Zhu, H Yuan, S Wang, J Liu, W Liu, C Deng\u2026", "- arXiv preprint arXiv", "\u2026, 2023 - arxiv.org"], "scholar_id": "JfAJ9q-AtsAJ", "snippet": "\u2026 as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (\u2026 language processing due to their remarkable language \u2026"}, {"title": "On the application of large language models for language teaching and assessment technology", "link": "https://arxiv.org/abs/2307.08393", "file_link": "https://arxiv.org/pdf/2307.08393", "authors": ["A Caines, L Benedetto, S Taslimipoor, C Davis\u2026", "- arXiv preprint arXiv", "\u2026, 2023 - arxiv.org"], "scholar_id": "O7O5dHnAyAMJ", "snippet": "\u2026 for incorporating large language models in AI-driven language teaching and \u2026 language learners. Overall we find that larger language models offer improvements over previous models \u2026"}, {"title": "A survey on large language models: Applications, challenges, limitations, and practical usage", "link": "https://www.authorea.com/doi/full/10.36227/techrxiv.23589741.v3", "file_link": null, "authors": ["MU Hadi, R Qureshi, A Shah, M Irfan, A Zafar\u2026", "- Authorea", "\u2026, 2023 - authorea.com"], "scholar_id": "GYdgocCnKXgJ", "snippet": "\u2026 Language Models (LLMs\u2026 Large language models (LLMs) are a type of artificial intelligence (AI) that have emerged as powerful tools for a wide range of tasks, including natural language \u2026"}, {"title": "[HTML][HTML] ChatGPT and other large language models are double-edged swords", "link": "https://pubs.rsna.org/doi/full/10.1148/radiol.230163", "file_link": "https://pubs.rsna.org/doi/full/10.1148/radiol.230163", "authors": ["Y Shen, L Heacock, J Elias, KD Hentel, B Reig, G Shih\u2026", "- Radiology, 2023 - pubs.rsna.org"], "scholar_id": "So0q8TRvxhYJ", "snippet": "\u2026 Large language models (LLMs) are deep learning models trained to understand and generate natural language. Recent studies demonstrated that LLMs achieve great success in a \u2026"}, {"title": "[PDF][PDF] Large language models in machine translation", "link": "https://aclanthology.org/D07-1090.pdf", "file_link": "https://aclanthology.org/D07-1090.pdf", "authors": ["T Brants, A Popat, P Xu, FJ Och\u2026", "- \u2026", "in Natural Language", "\u2026, 2007 - aclanthology.org"], "scholar_id": "sY5m_Y3-0Y4J", "snippet": "\u2026 the benefits of largescale statistical language modeling in ma\u2026 trillion tokens, resulting in language models having up to 300 \u2026 is inexpensive to train on large data sets and approaches the \u2026"}, {"title": "Autonomous chemical research with large language models", "link": "https://www.nature.com/articles/s41586-023-06792-0", "file_link": "https://www.nature.com/articles/s41586-023-06792-0.pdf", "authors": ["DA Boiko, R MacKnight, B Kline, G Gomes", "- Nature, 2023 - nature.com"], "scholar_id": "gzZFkshfYHAJ", "snippet": "\u2026 large language models are making significant strides in various fields, such as natural language \u2026 complex experiments by incorporating large language models empowered by tools \u2026"}, {"title": "Large language models as tool makers", "link": "https://arxiv.org/abs/2305.17126", "file_link": "https://arxiv.org/pdf/2305.17126", "authors": ["T Cai, X Wang, T Ma, X Chen, D Zhou", "- arXiv preprint arXiv:2305.17126, 2023 - arxiv.org"], "scholar_id": "vTrZSiGRMnwJ", "snippet": "\u2026 This paper explores the potential of enabling Large Language Models (LLMs) to create their own tools, thus allowing them greater autonomy in developing their ecosystem. While this \u2026"}]